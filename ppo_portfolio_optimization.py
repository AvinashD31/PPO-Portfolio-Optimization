# -*- coding: utf-8 -*-
"""PPO-Portfolio-Optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11I64W5oZxZg1B8g20t60AKLgQqYU1iKU
"""

!pip install yfinance gymnasium pandas numpy torch scikit-learn matplotlib

# ==============================================================================
# DEEP REINFORCEMENT LEARNING FOR PORTFOLIO OPTIMIZATION (PPO)
# "Google Antigravity" Implementation v1.0
# ==============================================================================

import numpy as np
import pandas as pd
import yfinance as yf
import gymnasium as gym
from gymnasium import spaces
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Normal
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# ==============================================================================
# 1. CONFIGURATION & HYPERPARAMETERS
# ==============================================================================
CONFIG = {
    "TICKERS": ["AAPL", "MSFT", "JPM", "GOOGL", "AMZN"],
    "START_DATE": "2015-01-01",
    "END_DATE": "2021-01-01",
    "INITIAL_BALANCE": 100000,
    "TRANSACTION_COST_BPS": 0.0002,  # 10 basis points (0.1%)
    "LOOKBACK_WINDOW": 50,          # Days of history the agent sees
    "TEST_SPLIT": 0.1,              # 20% data for testing

    # PPO Hyperparameters
    "LEARNING_RATE": 5e-5,
    "GAMMA": 0.99,
    "GAE_LAMBDA": 0.95,
    "CLIP_EPSILON": 0.1,
    "EPOCHS": 15,                   # Update epochs per rollout
    "BATCH_SIZE": 128,
    "TOTAL_TIMESTEPS": 150000,
}

# Device configuration (use GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ==============================================================================
# 2. DATA PIPELINE & FEATURE ENGINEERING
# ==============================================================================
def get_data(tickers, start, end):
    print(f"Downloading data for {tickers}...")
    # Add auto_adjust=False and multi_level=False to keep the old structure
    df = yf.download(tickers, start=start, end=end, progress=False, auto_adjust=False)

    # Check if 'Adj Close' exists (new yfinance might use 'Close' if auto_adjust is True)
    if 'Adj Close' in df.columns:
        df = df['Adj Close']
    else:
        df = df['Close']

    df = df.dropna()

    # Calculate Returns (for reward) and Technical Indicators (for state)
    data_dict = {}

    for ticker in tickers:
        # Check if the ticker is a column or a level in a multi-index
        series = df[ticker]

        # 1. Log Returns
        log_ret = np.log(series / series.shift(1))

        # 2. Rolling Volatility (21 days)
        vol = log_ret.rolling(window=21).std()

        # 3. RSI (Relative Strength Index)
        delta = series.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))

        # 4. MACD
        exp1 = series.ewm(span=12, adjust=False).mean()
        exp2 = series.ewm(span=26, adjust=False).mean()
        macd = exp1 - exp2

        # Combine features
        feat_df = pd.DataFrame({
            f'{ticker}_PRICE': series,
            f'{ticker}_RET': log_ret,
            f'{ticker}_VOL': vol,
            f'{ticker}_RSI': rsi,
            f'{ticker}_MACD': macd
        })
        data_dict[ticker] = feat_df

    # Merge and Clean
    full_df = pd.concat(data_dict.values(), axis=1).dropna()
    return full_df, df # Return feature matrix and raw price matrix

# ==============================================================================
# 3. CUSTOM GYMNASIUM ENVIRONMENT
# ==============================================================================
class PortfolioEnv(gym.Env):
    def __init__(self, data, price_data, tickers, mode='train'):
        super(PortfolioEnv, self).__init__()
        self.data = data.values
        self.price_data = price_data.values
        self.tickers = tickers
        self.n_assets = len(tickers)
        self.mode = mode

        # Define dimensions
        self.n_features = self.data.shape[1]

        # Action Space: Weights for each asset + Cash (Softmax normalized later)
        # We assume fully invested for simplicity, so action dim = n_assets
        self.action_space = spaces.Box(low=-1, high=1, shape=(self.n_assets,), dtype=np.float32)

        # Observation Space: [Market Features] + [Current Weights]
        self.obs_dim = self.n_features + self.n_assets
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_dim,), dtype=np.float32)

        # Split Index
        split_idx = int(len(data) * (1 - CONFIG['TEST_SPLIT']))
        if mode == 'train':
            self.start_step = 0
            self.end_step = split_idx
        else:
            self.start_step = split_idx
            self.end_step = len(data) - 1

        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = self.start_step
        self.balance = CONFIG['INITIAL_BALANCE']
        self.shares = np.zeros(self.n_assets)
        self.current_weights = np.zeros(self.n_assets)

        # Initial cash allocation (start with equal weights)
        prices = self.price_data[self.current_step]
        target_val = self.balance / self.n_assets
        self.shares = target_val / prices
        self.current_weights = (self.shares * prices) / self.balance

        self.portfolio_value = self.balance
        self.history = {'value': [], 'weights': []}

        return self._get_obs(), {}

    def _get_obs(self):
        market_state = self.data[self.current_step]
        state = np.concatenate((market_state, self.current_weights))
        return state.astype(np.float32)

    def step(self, action):
        # 1. Convert action to weights (Softmax)
        weights = np.exp(action) / np.sum(np.exp(action))

        # 2. Market Movement
        current_prices = self.price_data[self.current_step]
        next_prices = self.price_data[self.current_step + 1]

        # Calculate Price Return per asset
        asset_returns = (next_prices - current_prices) / current_prices

        # Portfolio Return (Weighted average of asset returns)
        portfolio_return = np.dot(weights, asset_returns)

        # 3. Transaction Costs (Deduct from return)
        cost = np.sum(np.abs(weights - self.current_weights)) * CONFIG['TRANSACTION_COST_BPS']
        net_return = portfolio_return - cost

        # 4. Update state
        self.portfolio_value *= (1 + net_return)
        self.current_weights = weights # Simplified for stable weight tracking

        # 5. REWARD DESIGN (The "Secret Sauce")
        # We penalize high variance. If the agent takes a huge risk for a small gain,
        # the reward becomes negative.
        risk_penalty = 0.5 * np.std(asset_returns)
        reward = net_return - risk_penalty

        # Scale reward for PPO stability (Deep learning likes values near -1 to 1)
        reward *= 10

        self.history['value'].append(self.portfolio_value)
        self.history['weights'].append(weights)

        self.current_step += 1
        terminated = self.current_step >= self.end_step - 1
        truncated = False

        return self._get_obs(), reward, terminated, truncated, {}

# ==============================================================================
# 4. PPO AGENT (PyTorch)
# ==============================================================================
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(ActorCritic, self).__init__()

        # Actor: Outputs the raw "preferences" for each asset
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, action_dim)
        )

        # Critic: Estimates the "value" (expected future reward) of the current state
        self.critic = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )

        # Learnable standard deviation for exploration
        self.log_std = nn.Parameter(torch.zeros(action_dim))

        # NEW: Orthogonal Initialization
        # This helps the gradients flow better and starts the agent with a
        # "neutral" portfolio instead of random extreme bets.
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            # Orthogonal init is a "Google Antigravity" trick for PPO stability
            torch.nn.init.orthogonal_(m.weight, gain=0.01)
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self, state):
        return self.actor(state), self.critic(state)

    def get_action(self, state):
        state = torch.FloatTensor(state).to(device)
        mean = self.actor(state)
        std = self.log_std.exp().expand_as(mean)
        dist = Normal(mean, std)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum(axis=-1)
        return action.cpu().numpy(), log_prob, self.critic(state)

    def evaluate(self, state, action):
        mean = self.actor(state)
        std = self.log_std.exp().expand_as(mean)
        dist = Normal(mean, std)
        log_prob = dist.log_prob(action).sum(axis=-1)
        entropy = dist.entropy().sum(axis=-1)
        value = self.critic(state)
        return log_prob, value.squeeze(), entropy

def compute_gae(rewards, values, masks, gamma=0.99, lam=0.95):
    returns = []
    gae = 0
    next_value = 0
    for step in reversed(range(len(rewards))):
        delta = rewards[step] + gamma * next_value * masks[step] - values[step]
        gae = delta + gamma * lam * masks[step] * gae
        returns.insert(0, gae + values[step])
        next_value = values[step]
    return returns


# ==============================================================================
# 5. MAIN EXECUTION LOOP
# ==============================================================================
if __name__ == "__main__":
    # A. Load Data
    full_data, price_data = get_data(CONFIG['TICKERS'], CONFIG['START_DATE'], CONFIG['END_DATE'])

    # Scale Data
    scaler = StandardScaler()
    full_data_scaled = pd.DataFrame(scaler.fit_transform(full_data), columns=full_data.columns)

    # B. Initialize Environment & Agent
    train_env = PortfolioEnv(full_data_scaled, price_data, CONFIG['TICKERS'], mode='train')
    test_env = PortfolioEnv(full_data_scaled, price_data, CONFIG['TICKERS'], mode='test')

    agent = ActorCritic(train_env.obs_dim, train_env.action_space.shape[0]).to(device)
    optimizer = optim.Adam(agent.parameters(), lr=CONFIG['LEARNING_RATE'])

    # C. Training Loop
    print("\nStarting Training...")
    state, _ = train_env.reset()

    # Storage for batch training
    states, actions, log_probs, rewards, values, masks = [], [], [], [], [], []

    for i in range(CONFIG['TOTAL_TIMESTEPS']):
        # Interact
        action, log_prob, value = agent.get_action(state)
        next_state, reward, done, _, _ = train_env.step(action)

        # Store
        states.append(state)
        actions.append(action)
        log_probs.append(log_prob.detach().cpu())
        rewards.append(reward)
        values.append(value.item())
        masks.append(1 - done)

        state = next_state

        if done:
            state, _ = train_env.reset()

        # Update PPO every BATCH_SIZE steps
        if (i+1) % 2048 == 0:
            next_val = agent.critic(torch.FloatTensor(state).to(device)).item()
            returns = compute_gae(rewards, values + [next_val], masks + [1])

            # Convert to Tensors
            t_states = torch.FloatTensor(np.array(states)).to(device)
            t_actions = torch.FloatTensor(np.array(actions)).to(device)
            t_old_log_probs = torch.stack(log_probs).to(device)
            t_returns = torch.FloatTensor(returns).to(device)
            t_values = torch.FloatTensor(values).to(device)
            t_advantages = t_returns - t_values

            # Optimize Surrogate Loss
            for _ in range(CONFIG['EPOCHS']):
                new_log_probs, new_values, entropy = agent.evaluate(t_states, t_actions)
                ratio = (new_log_probs - t_old_log_probs).exp()

                surr1 = ratio * t_advantages
                surr2 = torch.clamp(ratio, 1.0 - CONFIG['CLIP_EPSILON'], 1.0 + CONFIG['CLIP_EPSILON']) * t_advantages

                actor_loss = -torch.min(surr1, surr2).mean()
                critic_loss = 0.5 * (t_returns - new_values).pow(2).mean()

                loss = actor_loss + 0.5 * critic_loss - 0.05 * entropy.mean()

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            # Clear storage
            states, actions, log_probs, rewards, values, masks = [], [], [], [], [], []
            print(f"Step {i+1}: Updated Agent. Last Reward: {reward:.4f}")

    # D. Backtesting & Visualization
    print("\nBacktesting on Test Data...")
    state, _ = test_env.reset()
    done = False

    while not done:
        with torch.no_grad():
            action, _, _ = agent.get_action(state)
        state, _, done, _, _ = test_env.step(action)

    # Plot Results
    history = np.array(test_env.history['value'])
    plt.figure(figsize=(12,6))
    plt.plot(history, label='PPO Agent')

    # Calculate Buy & Hold Baseline (Equal Weights)
    test_prices = test_env.price_data[test_env.start_step:test_env.end_step]
    norm_prices = test_prices / test_prices[0]
    bnh_value = np.mean(norm_prices, axis=1) * CONFIG['INITIAL_BALANCE']
    plt.plot(bnh_value, label='Buy & Hold', linestyle='--')

    plt.title("Portfolio Optimization: PPO vs Buy-and-Hold")
    plt.xlabel("Trading Days")
    plt.ylabel("Portfolio Value ($)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Metrics
    agent_ret = (history[-1] - history[0]) / history[0]
    bnh_ret = (bnh_value[-1] - bnh_value[0]) / bnh_value[0]
    print(f"PPO Agent Return: {agent_ret*100:.2f}%")
    print(f"Buy & Hold Return: {bnh_ret*100:.2f}%")